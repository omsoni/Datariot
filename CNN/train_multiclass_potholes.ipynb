{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omsoni/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import potholes_dataset\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/omsoni/tensorflow/models/tutorials/image/imagenet/cv-tricks.com/Tensorflow-tutorials/tutorial-2-image-classifier'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "train_path = './data/Subset1-Simplex/'\n",
    "filename = 'simpleTrainAll.csv'\n",
    "\n",
    "#Prepare input data\n",
    "classes = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "#classes = np.array2string(classes)\n",
    "print(classes)\n",
    "num_classes = len(classes)\n",
    "# 20% of the data will automatically be used for validation\n",
    "validation_size = 0.2\n",
    "img_size = 128\n",
    "num_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = os.path.join(train_path, filename)\n",
    "#'./cv-tricks.com/Tensorflow-tutorials/tutorial-2-image-classifier/data/Subset1-Simplex/simpleTrainFullPhotosSortedFullAnnotations.csv'\n",
    "df = pd.read_csv(metadata_file, header=None)\n",
    "#path = \"./cv-tricks.com/Tensorflow-tutorials/tutorial-2-image-classifier/data/Subset1-Simplex/\"\n",
    "#print(df.head(5))\n",
    "files = df.iloc[:,0].map(str)\n",
    "labels = df.iloc[:,1].values\n",
    "#print('labels',labels)\n",
    "classes = pd.unique(labels)\n",
    "#print('classes', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Seed so that random initialization is consistent\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 0   1     2     3    4   5       6       7   \\\n",
      "0  Train-data\\Positive\\G0010033.jpg   6  1990  1406   66  14  1464.0  1442.0   \n",
      "1  Train-data\\Positive\\G0010034.jpg   5  1804  1424   90  16  1192.0  1484.0   \n",
      "2  Train-data\\Positive\\G0010035.jpg   4  1590  1452  106  24   840.0  1544.0   \n",
      "3  Train-data\\Positive\\G0010036.jpg   3  1322  1516  134  28   362.0  1658.0   \n",
      "4  Train-data\\Positive\\G0010037.jpg   2   884  1638  190  52  1428.0  1654.0   \n",
      "\n",
      "      8     9  ...  48  49  50  51  52  53  54  55  56  57  \n",
      "0   92.0  16.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "1  110.0  16.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "2  132.0  28.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "3  156.0  38.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "4   98.0  26.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "labels [6 5 4 ... 0 0 0]\n",
      "classes [ 6  5  4  3  2  7  9  8  1 10 11 14 12 13  0]\n",
      "WARNING:tensorflow:From <ipython-input-6-e3a69bb8ab79>:10: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "# We shall load all the training and validation images and labels into memory using openCV and use that during training\n",
    "data = potholes_dataset.read_train_sets(train_path, filename, img_size, validation_size=validation_size)\n",
    "#print('data {0}'.format(data.train.images))\n",
    "\n",
    "session = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size,img_size,num_channels], name='x')\n",
    "\n",
    "## labels\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Network graph params\n",
    "# Filter size 3x3\n",
    "filter_size_conv1 = 3 \n",
    "\n",
    "# 32 filters = 32 features ?\n",
    "num_filters_conv1 = 32\n",
    "\n",
    "# same as in layer 1\n",
    "filter_size_conv2 = 3\n",
    "num_filters_conv2 = 32\n",
    "\n",
    "# Same as in layer 2 but more filters why?\n",
    "filter_size_conv3 = 3\n",
    "num_filters_conv3 = 64\n",
    "    \n",
    "fc_layer_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def create_biases(size):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[size]))\n",
    "\n",
    "\n",
    "## Convulational Layer\n",
    "def create_convolutional_layer(input,\n",
    "               num_input_channels, \n",
    "               conv_filter_size,        \n",
    "               num_filters):  \n",
    "    \n",
    "    ## We shall define the weights that will be trained using create_weights function.\n",
    "    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n",
    "    ## We create biases using the create_biases function. These are also trained.\n",
    "    biases = create_biases(num_filters)\n",
    "\n",
    "    ## Creating the convolutional layer\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                     filter=weights,\n",
    "                     strides=[1, 1, 1, 1],\n",
    "                     padding='SAME')\n",
    "\n",
    "    layer += biases\n",
    "\n",
    "    ## We shall be using max-pooling.  \n",
    "    layer = tf.nn.max_pool(value=layer,\n",
    "                            ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "    ## Output of pooling is fed to Relu which is the activation function for us.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flatten_layer(layer):\n",
    "    #We know that the shape of the layer will be [batch_size img_size img_size num_channels] \n",
    "    # But let's get it from the previous layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    ## Number of features will be img_height * img_width* num_channels. But we shall calculate it in place of hard-coding it.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "\n",
    "    ## Now, we Flatten the layer so we shall have to reshape to num_features\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer\n",
    "\n",
    "## Fully Connected Layer\n",
    "def create_fc_layer(input,          \n",
    "             num_inputs,    \n",
    "             num_outputs,\n",
    "             use_relu=True):\n",
    "    \n",
    "    #Let's define trainable weights and biases.\n",
    "    weights = create_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = create_biases(num_outputs)\n",
    "\n",
    "    # Fully connected layer takes input x and produces wx+b.Since, these are matrices, we use matmul function in Tensorflow\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-4602c65ba3b1>:32: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_conv1 = create_convolutional_layer(input=x,\n",
    "               num_input_channels=num_channels,\n",
    "               conv_filter_size=filter_size_conv1,\n",
    "               num_filters=num_filters_conv1)\n",
    "layer_conv2 = create_convolutional_layer(input=layer_conv1,\n",
    "               num_input_channels=num_filters_conv1,\n",
    "               conv_filter_size=filter_size_conv2,\n",
    "               num_filters=num_filters_conv2)\n",
    "\n",
    "layer_conv3= create_convolutional_layer(input=layer_conv2,\n",
    "               num_input_channels=num_filters_conv2,\n",
    "               conv_filter_size=filter_size_conv3,\n",
    "               num_filters=num_filters_conv3)\n",
    "          \n",
    "layer_flat = create_flatten_layer(layer_conv3)\n",
    "\n",
    "layer_fc1 = create_fc_layer(input=layer_flat,\n",
    "                     num_inputs=layer_flat.get_shape()[1:4].num_elements(),\n",
    "                     num_outputs=fc_layer_size,\n",
    "                     use_relu=True)\n",
    "\n",
    "layer_fc2 = create_fc_layer(input=layer_fc1,\n",
    "                     num_inputs=fc_layer_size,\n",
    "                     num_outputs=num_classes,\n",
    "                     use_relu=False) \n",
    "\n",
    "y_pred = tf.nn.softmax(layer_fc2,name='y_pred')\n",
    "\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "session.run(tf.global_variables_initializer())\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                    labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "session.run(tf.global_variables_initializer()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "    val_acc = session.run(accuracy, feed_dict=feed_dict_validate)\n",
    "    msg = \"Training Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%},  Validation Loss: {3:.3f}\"\n",
    "    print(msg.format(epoch + 1, acc, val_acc, val_loss))\n",
    "\n",
    "total_iterations = 0\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_iteration):\n",
    "    global total_iterations\n",
    "    \n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iteration):\n",
    "\n",
    "        x_batch, y_true_batch,imgNames_batch = data.train.next_batch(batch_size)\n",
    "        #print('x_batch {0}, y_true_batch {1},imgNames_batch{2}', x_batch, y_true_batch,imgNames_batch)\n",
    "        x_valid_batch, y_valid_batch, imgNames_batch = data.valid.next_batch(batch_size)\n",
    "\n",
    "        \n",
    "        feed_dict_tr = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "        #print('feed_dict_tr{0}'.format(feed_dict_tr))\n",
    "        feed_dict_val = {x: x_valid_batch,\n",
    "                              y_true: y_valid_batch}\n",
    "\n",
    "        session.run(optimizer, feed_dict=feed_dict_tr)\n",
    "\n",
    "        if i % int(data.train.num_examples/batch_size) == 0: \n",
    "            val_loss = session.run(cost, feed_dict=feed_dict_val)\n",
    "            epoch = int(i / int(data.train.num_examples/batch_size))    \n",
    "            \n",
    "            show_progress(epoch, feed_dict_tr, feed_dict_val, val_loss)\n",
    "            saver.save(session, './SavedModel/multiclass_potholes_model') \n",
    "\n",
    "\n",
    "    total_iterations += num_iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 5.957\n",
      "Training Epoch 2 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 5.945\n",
      "Training Epoch 3 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.011\n",
      "Training Epoch 4 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.003\n",
      "Training Epoch 5 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.033\n",
      "Training Epoch 6 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.057\n",
      "Training Epoch 7 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.068\n",
      "Training Epoch 8 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.102\n",
      "Training Epoch 9 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.118\n",
      "Training Epoch 10 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.140\n",
      "Training Epoch 11 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.160\n",
      "Training Epoch 12 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.184\n",
      "Training Epoch 13 --- Training Accuracy: 100.0%, Validation Accuracy:  80.0%,  Validation Loss: 6.206\n"
     ]
    }
   ],
   "source": [
    "train(num_iteration=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
